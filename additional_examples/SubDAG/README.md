# A DAG within a DAG is a SubDAG

You may be wondering: can I start another DAG workflow inside of a different DAG workflow?
The answer is yes. 

When developing a complex DAG workflow, you may encounter the need
for a node in your workflow to submit a different DAG. In this case,
you might try defining

```
JOB AnotherDag another.dag
```

in your DAGMan input file. But this won't work!
Remember: the `JOB` syntax tells DAGMan to use `condor_submit` to submit the job.

To submit another DAG workflow, you need to have DAGMan use the `condor_submit_dag` command.
To do so, you need to use this syntax:

```
SUBDAG EXTERNAL <Node Name> <SubDAG Input>.dag
```

This "DAG within a DAG" approach is called a "SubDAG". 

The `SUBDAG` command will create a node named `<Node Name>` in the main DAG workflow.
For all intents and purposes, DAGMan will treat this like any other node:

* You can reference the SubDAG node in the PARENT/CHILD relationships. 
* The SubDAG node will not be submitted until the PARENT node(s) complete.
* The SubDAG node needs to complete successfully in order to submit its CHILD node(s).
* You can define `PRE` and `POST` scripts 
* You can define `RETRY` and other node settings

The only difference between a SubDAG node and a regular node is that DAGMan will use the `condor_submit_dag`
to submit the `<SubDAG Input>.dag` workflow.

A simple use case for a SubDAG is incorporating an existing DAG into
a larger workflow. In this case, the `.dag` file for the SubDAG already exists
and merely needs to be executed as part of the larger workflow.

Another, more complex use case is for submitting a SubDAG of arbitrary size, 
the size of which is not known when you submit the main DAG workflow.
The example below is one such case.

To understand the motivation for this use case, remember that all nodes for a DAG
must be declared in the input `.dag` file *at the time of submission*.
What if the first node of your DAG splits apart a large input file, but you don't
know ahead of time how many parts will be created?

Without the SubDAG feature, this would be an insurmountable problem.
The reason the SubDAG feature is useful here is that input files for a node
do not need to exist *until DAGMan tries to submit that node*.
Thus, an earlier node can create the input files (including the `.dag` file)
for a CHILD SubDAG node.

## Exercise

As with the other exercises, first examine the files and their contents and
try to identify the structure of the DAG workflow.

In this example, the main DAG input file `sample.dag` declares the 
following nodes:

* `split` 
* `my_subdag` 
* `recombine`

where the `split` node is the PARENT of the `my_subdag` node, which in
turn is the PARENT of the `recombine` node. While three nodes are declared,
only the submit files for the `split` and `recombine` nodes exist at the start!

Can you figure out how the files for `my_subdag` node will be created?

If you look into `split` node files, you'll see the executable file `split.py`.
This is a Python script that creates the file `my_subdag.dag` with an arbitrary number 
(between 1 and 25) of `JOB` node declarations, each of which call `echo.sub`. 
Thus, when the `JOB` attached to the `split` node completes, it will return 
the input `my_subdag.dag` file needed to execute the next node in `sample.dag`. 

(In this example, `echo.sub` already exists, but it's easy to imagine extending 
the script to automatically generate that file or even a unique submit file for 
each node. Also, a Python script was used here for easy generation of a random 
number.)

Next, examine the contents of `echo.sub` and `echo.sh`. 

* What does this job do?
* What will happen when this job is executed multiple times as part of the SubDAG `my_subdag`?

Similarly, examine the contents of `recombine.sub` and its executable `recombine.sh`. 

The `echo` jobs will return a message in `echo.####.out` in the
`./echo` folder, and the `recombine` job will combine these messages and 
return a single file called `recombine.out` in the `out/` folder.
Since the `echo` jobs are run as part of the `my_subdag` node, the `recombine` job won't start until they have
all completed.

To see this in action, submit `sample.dag` without any modifications:

```
$ condor_submit_dag sample.dag
```

As the job progresses, 

* `my_subdag.dag` will be automatically created (returned as output of the `split` node)
* `my_subdag.dag` will be automatically submitted
* The results generated by the SubDAG will be recombined 

Once the DAG completes, there should be a file `recombine.out` in the `out/` folder that contains the job
numbers for the `echo` jobs submitted as part of the auto-generated `my_subdag.dag`.

Explore the various files and logs and confirm that your understanding of the
workflow and its execution is correct.

> As you can see, a lot of files were generated in this process. As an additional
> exercise, consider resetting the example and then modifying the `.dag`, `.sub`, 
> and executable files to organize the various results/steps into individual 
> directories. The following command should reset this example to its original
> state:
>
> ```
> rm echo/* log/* out/* err/* my_subdag.dag* sample.dag.*
> ```

* For more information on the `SUBDAG` utility, see the
  [DAGMan SubDAG Documentation](https://htcondor.readthedocs.io/en/latest/automated-workflows/dagman-using-other-dags.html#a-dag-within-a-dag-is-a-subdag).

